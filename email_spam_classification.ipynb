{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "email spam classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZNhPIvg0CUhci72qwiSCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeKMaina/knn-naive-bayes/blob/main/email_spam_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMAIL SPAM CLASSIFICATION USING NAIVE BAYES"
      ],
      "metadata": {
        "id": "OARzt2zcO6Xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Defining the question\n"
      ],
      "metadata": {
        "id": "xGOHI6E-PLSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Given the data, we are to predict whether an email is spam or not by building a Naive Bayes Model."
      ],
      "metadata": {
        "id": "cQb9DNItPUxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Metric for success"
      ],
      "metadata": {
        "id": "QuQA8G5cPLej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Our analysis will be considered successful if we are able to develop a Naive Bayes model that can accurately predict whether or not an email is spam."
      ],
      "metadata": {
        "id": "ODasc7uDPg9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Understanding the context"
      ],
      "metadata": {
        "id": "CfOrrWSCPLoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The spam dataset's final column indicates whether the e-mail was considered spam (1) or not (0). The majority of the attributes indicate whether a specific word or character appeared frequently in the e-mail."
      ],
      "metadata": {
        "id": "SIKzsdzlPx4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Experimental design"
      ],
      "metadata": {
        "id": "7UiAXG0lPLzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data Preparation\n",
        "- Exploratory Data Analysis\n",
        "- Data Preprocessing\n",
        "- Building our models: Gaussian,Multinomial\n",
        "- Challenging the solutions\n",
        "- Conclusion and Recommendations"
      ],
      "metadata": {
        "id": "o0lWwvzUP8od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries and the data we will use"
      ],
      "metadata": {
        "id": "AzHaZCxiQErD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r2voehLB93k6"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from pandas_profiling import ProfileReport\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Set global parameters\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading our datasets\n",
        "with open('/content/spambase.names') as file:\n",
        "  names = file.read()\n",
        "  print(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK649UnZ_yc5",
        "outputId": "eabe546e-4500-4641-aace-00096f6e3a49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| SPAM E-MAIL DATABASE ATTRIBUTES (in .names format)\n",
            "|\n",
            "| 48 continuous real [0,100] attributes of type word_freq_WORD \n",
            "| = percentage of words in the e-mail that match WORD,\n",
            "| i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
            "| total number of words in e-mail.  A \"word\" in this case is any \n",
            "| string of alphanumeric characters bounded by non-alphanumeric \n",
            "| characters or end-of-string.\n",
            "|\n",
            "| 6 continuous real [0,100] attributes of type char_freq_CHAR\n",
            "| = percentage of characters in the e-mail that match CHAR,\n",
            "| i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
            "|\n",
            "| 1 continuous real [1,...] attribute of type capital_run_length_average\n",
            "| = average length of uninterrupted sequences of capital letters\n",
            "|\n",
            "| 1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
            "| = length of longest uninterrupted sequence of capital letters\n",
            "|\n",
            "| 1 continuous integer [1,...] attribute of type capital_run_length_total\n",
            "| = sum of length of uninterrupted sequences of capital letters\n",
            "| = total number of capital letters in the e-mail\n",
            "|\n",
            "| 1 nominal {0,1} class attribute of type spam\n",
            "| = denotes whether the e-mail was considered spam (1) or not (0), \n",
            "| i.e. unsolicited commercial e-mail.  \n",
            "|\n",
            "| For more information, see file 'spambase.DOCUMENTATION' at the\n",
            "| UCI Machine Learning Repository: http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
            "\n",
            "\n",
            "1, 0.    | spam, non-spam classes\n",
            "\n",
            "word_freq_make:         continuous.\n",
            "word_freq_address:      continuous.\n",
            "word_freq_all:          continuous.\n",
            "word_freq_3d:           continuous.\n",
            "word_freq_our:          continuous.\n",
            "word_freq_over:         continuous.\n",
            "word_freq_remove:       continuous.\n",
            "word_freq_internet:     continuous.\n",
            "word_freq_order:        continuous.\n",
            "word_freq_mail:         continuous.\n",
            "word_freq_receive:      continuous.\n",
            "word_freq_will:         continuous.\n",
            "word_freq_people:       continuous.\n",
            "word_freq_report:       continuous.\n",
            "word_freq_addresses:    continuous.\n",
            "word_freq_free:         continuous.\n",
            "word_freq_business:     continuous.\n",
            "word_freq_email:        continuous.\n",
            "word_freq_you:          continuous.\n",
            "word_freq_credit:       continuous.\n",
            "word_freq_your:         continuous.\n",
            "word_freq_font:         continuous.\n",
            "word_freq_000:          continuous.\n",
            "word_freq_money:        continuous.\n",
            "word_freq_hp:           continuous.\n",
            "word_freq_hpl:          continuous.\n",
            "word_freq_george:       continuous.\n",
            "word_freq_650:          continuous.\n",
            "word_freq_lab:          continuous.\n",
            "word_freq_labs:         continuous.\n",
            "word_freq_telnet:       continuous.\n",
            "word_freq_857:          continuous.\n",
            "word_freq_data:         continuous.\n",
            "word_freq_415:          continuous.\n",
            "word_freq_85:           continuous.\n",
            "word_freq_technology:   continuous.\n",
            "word_freq_1999:         continuous.\n",
            "word_freq_parts:        continuous.\n",
            "word_freq_pm:           continuous.\n",
            "word_freq_direct:       continuous.\n",
            "word_freq_cs:           continuous.\n",
            "word_freq_meeting:      continuous.\n",
            "word_freq_original:     continuous.\n",
            "word_freq_project:      continuous.\n",
            "word_freq_re:           continuous.\n",
            "word_freq_edu:          continuous.\n",
            "word_freq_table:        continuous.\n",
            "word_freq_conference:   continuous.\n",
            "char_freq_;:            continuous.\n",
            "char_freq_(:            continuous.\n",
            "char_freq_[:            continuous.\n",
            "char_freq_!:            continuous.\n",
            "char_freq_$:            continuous.\n",
            "char_freq_#:            continuous.\n",
            "capital_run_length_average: continuous.\n",
            "capital_run_length_longest: continuous.\n",
            "capital_run_length_total:   continuous.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets get our column names and enter them in a list from the data above\n",
        "columns = ['word_freq_make',\n",
        "          'word_freq_address',      \n",
        "          'word_freq_all',          \n",
        "          'word_freq_3d',          \n",
        "          'word_freq_our',          \n",
        "          'word_freq_over',         \n",
        "          'word_freq_remove',       \n",
        "          'word_freq_internet',     \n",
        "          'word_freq_order',        \n",
        "          'word_freq_mail',         \n",
        "          'word_freq_receive',      \n",
        "          'word_freq_will',         \n",
        "          'word_freq_people',       \n",
        "          'word_freq_report',       \n",
        "          'word_freq_addresses',    \n",
        "          'word_freq_free',         \n",
        "          'word_freq_business',     \n",
        "          'word_freq_email',        \n",
        "          'word_freq_you',          \n",
        "          'word_freq_credit',       \n",
        "          'word_freq_your',         \n",
        "          'word_freq_font',         \n",
        "          'word_freq_000',          \n",
        "          'word_freq_money',        \n",
        "          'word_freq_hp',           \n",
        "          'word_freq_hpl',          \n",
        "          'word_freq_george',       \n",
        "          'word_freq_650',          \n",
        "          'word_freq_lab',          \n",
        "          'word_freq_labs',         \n",
        "          'word_freq_telnet',       \n",
        "          'word_freq_857',          \n",
        "          'word_freq_data',         \n",
        "          'word_freq_415',          \n",
        "          'word_freq_85',           \n",
        "          'word_freq_technology',   \n",
        "          'word_freq_1999',         \n",
        "          'word_freq_parts',        \n",
        "          'word_freq_pm',           \n",
        "          'word_freq_direct',       \n",
        "          'word_freq_cs',           \n",
        "          'word_freq_meeting',      \n",
        "          'word_freq_original',     \n",
        "          'word_freq_project',      \n",
        "          'word_freq_re',           \n",
        "          'word_freq_edu',          \n",
        "          'word_freq_table',        \n",
        "          'word_freq_conference',   \n",
        "          'char_freq_;',            \n",
        "          'char_freq_(',            \n",
        "          'char_freq_[',            \n",
        "          'char_freq_!',            \n",
        "          'char_freq_$',            \n",
        "          'char_freq_#',            \n",
        "          'capital_run_length_average', \n",
        "          'capital_run_length_longest', \n",
        "          'capital_run_length_total',\n",
        "          'spam']"
      ],
      "metadata": {
        "id": "7pZRHzkiBF3-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email = pd.read_csv('/content/spambase.data', names=columns)"
      ],
      "metadata": {
        "id": "Sf-Dp9eV99Ll"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the top of our data\n",
        "email.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "lWBVOk3BB9T8",
        "outputId": "7746e051-76af-4605-baf6-7fe40c5bed35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "0            0.00               0.64  ...                       278     1\n",
              "1            0.21               0.28  ...                      1028     1\n",
              "2            0.06               0.00  ...                      2259     1\n",
              "3            0.00               0.00  ...                       191     1\n",
              "4            0.00               0.00  ...                       191     1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preview the bottom of our data\n",
        "email.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Uq6AI_r0B_p-",
        "outputId": "051afa38-9de7-4be2-f1ee-94850c654c15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.147</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4600</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.97</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "4596            0.31                0.0  ...                        88     0\n",
              "4597            0.00                0.0  ...                        14     0\n",
              "4598            0.30                0.0  ...                       118     0\n",
              "4599            0.96                0.0  ...                        78     0\n",
              "4600            0.00                0.0  ...                        40     0\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the shape of our data\n",
        "email.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ2odQzvCGle",
        "outputId": "0791eb45-12e6-4920-e4e7-1f1e213b8ed0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4601, 58)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the dtypes of our columns\n",
        "email.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLAx6VvMCI85",
        "outputId": "99cc86c1-b9e3-4365-c833-52e168196dfb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4601 entries, 0 to 4600\n",
            "Data columns (total 58 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   word_freq_make              4601 non-null   float64\n",
            " 1   word_freq_address           4601 non-null   float64\n",
            " 2   word_freq_all               4601 non-null   float64\n",
            " 3   word_freq_3d                4601 non-null   float64\n",
            " 4   word_freq_our               4601 non-null   float64\n",
            " 5   word_freq_over              4601 non-null   float64\n",
            " 6   word_freq_remove            4601 non-null   float64\n",
            " 7   word_freq_internet          4601 non-null   float64\n",
            " 8   word_freq_order             4601 non-null   float64\n",
            " 9   word_freq_mail              4601 non-null   float64\n",
            " 10  word_freq_receive           4601 non-null   float64\n",
            " 11  word_freq_will              4601 non-null   float64\n",
            " 12  word_freq_people            4601 non-null   float64\n",
            " 13  word_freq_report            4601 non-null   float64\n",
            " 14  word_freq_addresses         4601 non-null   float64\n",
            " 15  word_freq_free              4601 non-null   float64\n",
            " 16  word_freq_business          4601 non-null   float64\n",
            " 17  word_freq_email             4601 non-null   float64\n",
            " 18  word_freq_you               4601 non-null   float64\n",
            " 19  word_freq_credit            4601 non-null   float64\n",
            " 20  word_freq_your              4601 non-null   float64\n",
            " 21  word_freq_font              4601 non-null   float64\n",
            " 22  word_freq_000               4601 non-null   float64\n",
            " 23  word_freq_money             4601 non-null   float64\n",
            " 24  word_freq_hp                4601 non-null   float64\n",
            " 25  word_freq_hpl               4601 non-null   float64\n",
            " 26  word_freq_george            4601 non-null   float64\n",
            " 27  word_freq_650               4601 non-null   float64\n",
            " 28  word_freq_lab               4601 non-null   float64\n",
            " 29  word_freq_labs              4601 non-null   float64\n",
            " 30  word_freq_telnet            4601 non-null   float64\n",
            " 31  word_freq_857               4601 non-null   float64\n",
            " 32  word_freq_data              4601 non-null   float64\n",
            " 33  word_freq_415               4601 non-null   float64\n",
            " 34  word_freq_85                4601 non-null   float64\n",
            " 35  word_freq_technology        4601 non-null   float64\n",
            " 36  word_freq_1999              4601 non-null   float64\n",
            " 37  word_freq_parts             4601 non-null   float64\n",
            " 38  word_freq_pm                4601 non-null   float64\n",
            " 39  word_freq_direct            4601 non-null   float64\n",
            " 40  word_freq_cs                4601 non-null   float64\n",
            " 41  word_freq_meeting           4601 non-null   float64\n",
            " 42  word_freq_original          4601 non-null   float64\n",
            " 43  word_freq_project           4601 non-null   float64\n",
            " 44  word_freq_re                4601 non-null   float64\n",
            " 45  word_freq_edu               4601 non-null   float64\n",
            " 46  word_freq_table             4601 non-null   float64\n",
            " 47  word_freq_conference        4601 non-null   float64\n",
            " 48  char_freq_;                 4601 non-null   float64\n",
            " 49  char_freq_(                 4601 non-null   float64\n",
            " 50  char_freq_[                 4601 non-null   float64\n",
            " 51  char_freq_!                 4601 non-null   float64\n",
            " 52  char_freq_$                 4601 non-null   float64\n",
            " 53  char_freq_#                 4601 non-null   float64\n",
            " 54  capital_run_length_average  4601 non-null   float64\n",
            " 55  capital_run_length_longest  4601 non-null   int64  \n",
            " 56  capital_run_length_total    4601 non-null   int64  \n",
            " 57  spam                        4601 non-null   int64  \n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "kKh9UhuqCY0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for null values\n",
        "email.isnull().sum().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4EEnJWJCamo",
        "outputId": "d69554b5-f86b-48fa-c5dd-6769730b7fcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cheking our column names\n",
        "#\n",
        "email.columns\n",
        "\n",
        "#all names are in lower case and have been separated by an underscore('_')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUu4Ira4Cmf2",
        "outputId": "30fad24e-b29d-4851-a99d-74ba9f4c0cb1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
              "       'word_freq_our', 'word_freq_over', 'word_freq_remove',\n",
              "       'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
              "       'word_freq_receive', 'word_freq_will', 'word_freq_people',\n",
              "       'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
              "       'word_freq_business', 'word_freq_email', 'word_freq_you',\n",
              "       'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000',\n",
              "       'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
              "       'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet',\n",
              "       'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
              "       'word_freq_technology', 'word_freq_1999', 'word_freq_parts',\n",
              "       'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
              "       'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
              "       'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
              "       'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
              "       'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
              "       'capital_run_length_longest', 'capital_run_length_total', 'spam'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicates\n",
        "email.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABAy5M_sCdDl",
        "outputId": "662ced16-6e01-4c62-e925-b8d8d36c420c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping our duplicates\n",
        "email.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "xz77CO_pQijW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "rIf9QG7oDCGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the target variable \n",
        "\n",
        "sns.countplot(email.spam)\n",
        "plt.title('Spam vs Non_Spam Emails')\n",
        "plt.xticks([0,1],['Not Spam', 'Spam'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "0dLkGz2bDDBY",
        "outputId": "6da8b0ff-13c3-47f1-b734-b90a96ac21ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check proportion of target variable in %\n",
        "email.spam.value_counts(normalize=True)*100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-jBKnmnDVDq",
        "outputId": "0fa03fe5-d3de-4f50-be7a-7310edcb423c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    60.118765\n",
              "1    39.881235\n",
              "Name: spam, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "60.1% of our emails are normal(not spam) and 39.9% are spam emails."
      ],
      "metadata": {
        "id": "zYsAtDKtDTyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelling"
      ],
      "metadata": {
        "id": "wGUiCQCUD0x9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline model"
      ],
      "metadata": {
        "id": "xEaoGru4E91c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our predictor and target variables\n",
        "#\n",
        "X = email.drop(['spam'], axis = 1)\n",
        "y = email['spam']\n",
        "\n",
        "# Spliting our dataset\n",
        "#\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 0, test_size = 0.2)\n",
        "# Scaling predictor variables\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "# Fitting the data\n",
        "regressor = LogisticRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_XAFO6mE_hW",
        "outputId": "a7600847-ba01-4b90-98f7-b2a0d7fcb260"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the prediction. \n",
        "#\n",
        "y_pred = regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "kolQLDI_FnfY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the score of the baseline model. \n",
        "#\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('The confusuion matrix is: ', \"\\n\",cm)\n",
        "print ('The accuracy score: ', accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRBmMourFsDE",
        "outputId": "08b021fc-7569-4c8c-ef5f-329e0d5e73bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error: 0.3043625791263548\n",
            "Mean Squared Error: 0.09263657957244656\n",
            "The confusuion matrix is:  \n",
            " [[465  30]\n",
            " [ 48 299]]\n",
            "The accuracy score:  0.9073634204275535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- our baseline model has an RMSE of 0.31 which is relatively low\n",
        "- The accuracy of our baseline model is 91%"
      ],
      "metadata": {
        "id": "AjQ4U1XYGGXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive bayes model"
      ],
      "metadata": {
        "id": "OyGjxaqCGZUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our data is continous, we will use the gaussian naive bayes model. The assumption is that the data is normally distributed"
      ],
      "metadata": {
        "id": "58AAjiTKHGq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting our model\n",
        "#\n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "TLDH-Dy_GbU2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting our test predictors\n",
        "predicted = model.predict(X_test)\n",
        "print ('The accuracy score: ', accuracy_score(y_test, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXHPzvANGQb2",
        "outputId": "16de2c77-6f82-4027-ed8e-281b27935ea1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy score:  0.8171021377672208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the parameters to be hypertuned\n",
        "GaussianNB()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIISAYIYDpVl",
        "outputId": "4a52fa3a-3625-4e61-e7cd-c23571065b4d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# performing gridsearch \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "np.random.seed(999)\n",
        "\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "params_ = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "\n",
        "cv = RepeatedStratifiedKFold(n_splits=15, \n",
        "                                    n_repeats=3, \n",
        "                                    random_state=0)\n",
        "\n",
        "gs_ = GridSearchCV(estimator=nb_classifier, \n",
        "                     param_grid=params_, \n",
        "                     cv=cv,\n",
        "                     verbose=1, \n",
        "                     scoring='accuracy')\n",
        "\n",
        "gs_.fit(X_train, y_train)\n",
        "\n",
        "gs_.best_params_\n",
        "print('best parameters:',gs_.best_params_)\n",
        "print('best score:',gs_.best_score_)\n",
        "print('best estimator:',gs_.best_estimator_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQYohBkKHgN_",
        "outputId": "59ceaaea-276f-4aa4-9c91-e6ec47c45cc1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 45 folds for each of 100 candidates, totalling 4500 fits\n",
            "best parameters: {'var_smoothing': 4.328761281083062e-05}\n",
            "best score: 0.8211644620811288\n",
            "best estimator: GaussianNB(var_smoothing=4.328761281083062e-05)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's apply the best params\n",
        "# Splitting the data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Training our model\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "clf = GaussianNB(priors=None,var_smoothing= 4.328761281083062e-05)\n",
        "model = clf.fit(X_train, y_train) \n",
        "\n",
        "# Predicting our test predictors\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import scipy.stats as stats\n",
        "print(\"Naive bayes model accuracy(70-30 split) is\", accuracy_score(y_test,y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gAXFrwuHgPe",
        "outputId": "f03e72fd-c0eb-4c40-c833-75b252b2d0c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive bayes model accuracy(70-30 split) is 0.6611243072050673\n",
            "[[715  22]\n",
            " [406 120]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.97      0.77       737\n",
            "           1       0.85      0.23      0.36       526\n",
            "\n",
            "    accuracy                           0.66      1263\n",
            "   macro avg       0.74      0.60      0.56      1263\n",
            "weighted avg       0.72      0.66      0.60      1263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The model's accuracy reduced from an accuracy score of 81.7% to 66.9%."
      ],
      "metadata": {
        "id": "2GfFOYTBIlrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing LDA"
      ],
      "metadata": {
        "id": "vdlrqAiVI8b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-LDA performs dimensionality reduction by choosing the variables that can explain our taget variable the best"
      ],
      "metadata": {
        "id": "lLcEiNWlI-r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting our train and test set \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=0)\n",
        "\n",
        "# Scaling our features.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# LDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "lda = LDA()\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "# Training our model\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "clf = GaussianNB()\n",
        "model = clf.fit(X_train, y_train) \n",
        "\n",
        "# Predicting \n",
        "y_pred_lda = model.predict(X_test)\n",
        "\n",
        "# evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import scipy.stats as stats\n",
        "print(\"The model accuracy is\", accuracy_score(y_test,y_pred_lda))\n",
        "print(confusion_matrix(y_test, y_pred_lda))\n",
        "print(classification_report(y_test, y_pred_lda))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9ZUX5-7HgUX",
        "outputId": "849623bf-e39c-48ef-d750-bfb64a9d03fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model accuracy is 0.9073634204275535\n",
            "[[693  44]\n",
            " [ 73 453]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92       737\n",
            "           1       0.91      0.86      0.89       526\n",
            "\n",
            "    accuracy                           0.91      1263\n",
            "   macro avg       0.91      0.90      0.90      1263\n",
            "weighted avg       0.91      0.91      0.91      1263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After performing LDA,the accuracy score yielded was 90.73% which is the best performance overall."
      ],
      "metadata": {
        "id": "YI2f0PUQOSzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the LDA coeficients\n",
        "factors = pd.DataFrame (index = X.columns.values, data = lda.coef_[0].T)\n",
        "factors.sort_values(0, ascending = False).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zJcEc7CLOgjn",
        "outputId": "ac66617a-fa7f-4fa1-f9e4-dcd9a2817e9e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>word_freq_remove</th>\n",
              "      <td>0.798490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>char_freq_!</th>\n",
              "      <td>0.731173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_freq_your</th>\n",
              "      <td>0.586018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_freq_our</th>\n",
              "      <td>0.584870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_freq_free</th>\n",
              "      <td>0.573864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_freq_000</th>\n",
              "      <td>0.551253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>char_freq_$</th>\n",
              "      <td>0.540619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_freq_font</th>\n",
              "      <td>0.463493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <td>0.433138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word_freq_money</th>\n",
              "      <td>0.411027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 0\n",
              "word_freq_remove          0.798490\n",
              "char_freq_!               0.731173\n",
              "word_freq_your            0.586018\n",
              "word_freq_our             0.584870\n",
              "word_freq_free            0.573864\n",
              "word_freq_000             0.551253\n",
              "char_freq_$               0.540619\n",
              "word_freq_font            0.463493\n",
              "capital_run_length_total  0.433138\n",
              "word_freq_money           0.411027"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenging the solution"
      ],
      "metadata": {
        "id": "vtJy80p9TBW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and test sets,\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling our features.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Training our model \n",
        "svm = SVC(C=0.1, gamma=0.001, kernel = 'linear')\n",
        "model = svm.fit(X_train, y_train) \n",
        "\n",
        "# Predicting \n",
        "y_pred_svm = model.predict(X_test)\n",
        "\n",
        "# evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import scipy.stats as stats\n",
        "print(\"The model accuracy is\", accuracy_score(y_test,y_pred_svm))\n",
        "print(confusion_matrix(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6eCdyuaSDYS",
        "outputId": "9f2c01e1-d71d-4b41-f9e7-aa37f58f8e58"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model accuracy is 0.9144893111638955\n",
            "[[698  42]\n",
            " [ 66 457]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.93       740\n",
            "           1       0.92      0.87      0.89       523\n",
            "\n",
            "    accuracy                           0.91      1263\n",
            "   macro avg       0.91      0.91      0.91      1263\n",
            "weighted avg       0.91      0.91      0.91      1263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The svm algorithim produce an accuracy score of 91.44% which is negligibly higher than that of the naive bayes algorithim with LDA (90.7%)."
      ],
      "metadata": {
        "id": "ZMcluH4XSvY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "VHHZVbXDTGsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The naive bayes algorithim with LDA is sufficient as it gives a score close to that of svm"
      ],
      "metadata": {
        "id": "DcZ-p9-xTT87"
      }
    }
  ]
}